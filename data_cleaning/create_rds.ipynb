{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import time\n",
    "import pandas as pd\n",
    "import sqlite3\n",
    "import os\n",
    "from bs4 import BeautifulSoup\n",
    "import boto3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def start_scraping(patent_no, patent_url):\n",
    "    \n",
    "    # Use requests to fetch web content\n",
    "    try:\n",
    "        response = requests.get(patent_url)\n",
    "    except:\n",
    "        file_name = f\"/failed_patent_no.txt\"\n",
    "        with open(file_name, 'w') as file:\n",
    "            f_result = str(patent_no) + \",\"\n",
    "            file.write(f_result)\n",
    "    \n",
    "    # Check if the request was successful\n",
    "    if response.status_code == 200:\n",
    "        # Retrieve the content\n",
    "        content = response.text\n",
    "        \n",
    "        # Create a filename and specify the /tmp directory\n",
    "        file_name = f\"raw/{patent_no}.txt\"\n",
    "        \n",
    "        # Create a file object in the /tmp directory\n",
    "        with open(file_name, 'w') as file:\n",
    "            file.write(content)\n",
    "    else:\n",
    "        print(f\"Failed to retrieve content from {patent_url}, status code: {response.status_code}\")\n",
    "\n",
    "\n",
    "# Get the list of patent numbers and URLs from the event\n",
    "patent_nos = ['US-2019018692-A1', 'US-2019005195-A1']\n",
    "\n",
    "patent_urls = ['https://patents.google.com/patent/US20190018692A1/en','https://patents.google.com/patent/US20190005195A1/en']\n",
    "\n",
    "# Iterate through the list of patent_nos and patent_urls\n",
    "for patent_no, patent_url in zip(patent_nos, patent_urls):\n",
    "    # Call start_scraping function for each pair of patent_no and patent_url\n",
    "    start_scraping(patent_no, patent_url)\n",
    "    time.sleep(2)  # Pause for 2 seconds between requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set AWS access key and region\n",
    "access_key = 'ASIA4HHT4VBDMJO2QF4G'\n",
    "secret_key = 'p+sCMjycxCeSjZ+FT57gl8+JxM0JnEU55sRjo7SG'\n",
    "token = 'IQoJb3JpZ2luX2VjEP3//////////wEaCXVzLXdlc3QtMiJHMEUCIQDZo+Psf3UInSyYtJklRwtSxondJBO+VcSidx9jykZOkwIgQG1ebhQWh6cEGom0fOXkJ+GYGjZdOhqovGLJgY9hYRUqsAIIZhABGgw4NDAxNzc1OTIzOTAiDKEV49k5obxFfEZFqiqNAtzac/dI1vGFjpr2I6IjogFP8I4SpcmBBJfd9b7cpc7XpzNT8/lIwm4bKMlW2kLIYBME2m5WLTY8PCp1yi9jeUw9U6dV2GmQU29UGBN6nJNpwbVNdLxu0szxZDtmnssoT2Xf+Ny6ud5bfreYTGM1R86CfMTbYSP2GT5JIav27Acgqwi175g4WxSh2yam9yWqe70bG75XYaM8cRVtH74qdtcjqvPPu5TMmm07VAuWK98QTw0ZzMYQFs6w/ijgZ1l7YeeN/ZJFVnmGpbbA+Z9Cgc9TeiMQtn9UggenmrgRBWRhqV+eVswgk1aN8/zUTxD8bDihQ2n/XbP2AjszlBqbFK7X1GzPgxnmqv7ov4wHMLfYhLIGOp0Bqjho/f1remTOmNFzR62srQLyDlozy4+BB3iZ1i19v9NfKOOQ5rIUUbXSCutB5hyNoGnJXUN2Kw/A8iXNZlh1uZ56Nj47D8kYFXkSieOnNk9aZuG1YzTfvMUHiBogNsCQzpj8JQdJQlVyjNNm+bUu98FmJJiaPTRdKy5w3nzAlCPZtE50Cgdw+fAQmwZtN9BsFcVsDAIou3SuinJ6nA=='\n",
    "\n",
    "# Create S3 client\n",
    "s3 = boto3.client('s3', aws_access_key_id=access_key, aws_secret_access_key=secret_key, aws_session_token=token)\n",
    "\n",
    "# Set S3 bucket and folder name\n",
    "bucket_name = 'patent-bucket-raw'\n",
    "\n",
    "# Local save path\n",
    "local_folder = 'raw'\n",
    "\n",
    "# Ensure local save path exists\n",
    "if not os.path.exists(local_folder):\n",
    "    os.makedirs(local_folder)\n",
    "\n",
    "# Download the first 100 files to the local path\n",
    "max_files = 100\n",
    "count = 0\n",
    "for obj in s3.list_objects_v2(Bucket=bucket_name)['Contents']:\n",
    "    key = obj['Key']\n",
    "    file_name = os.path.join(local_folder, key.split('/')[-1])  # Specify file name and save path\n",
    "    s3.download_file(bucket_name, key, file_name)\n",
    "    count += 1\n",
    "    if count >= max_files:\n",
    "        break\n",
    "\n",
    "print(\"The first 100 files have been downloaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# connect to the database\n",
    "conn = sqlite3.connect('patent.db')\n",
    "cursor = conn.cursor()\n",
    "\n",
    "# delete all tables\n",
    "cursor.execute(\"SELECT name FROM sqlite_master WHERE type='table';\")\n",
    "tables = cursor.fetchall()\n",
    "for table in tables:\n",
    "    cursor.execute(\"DROP TABLE IF EXISTS {}\".format(table[0]))\n",
    "\n",
    "# create a new table\n",
    "cursor.execute('''\n",
    "    CREATE TABLE IF NOT EXISTS patent_data (\n",
    "        id CHAR(30) PRIMARY KEY,\n",
    "        abstract TEXT,\n",
    "        classification TEXT,\n",
    "        timeline TEXT,\n",
    "        citedby TEXT,\n",
    "        legal TEXT\n",
    "    );\n",
    "''')\n",
    "\n",
    "# close the connection\n",
    "conn.commit()\n",
    "conn.close()\n",
    "\n",
    "print(\"database created successfully\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conn = sqlite3.connect('patent.db')\n",
    "cursor = conn.cursor()\n",
    "\n",
    "# loop through all files in the raw folder\n",
    "folder_path = 'raw'\n",
    "for file_name in os.listdir(folder_path):\n",
    "    if file_name.endswith('.txt'):\n",
    "        file_path = os.path.join(folder_path, file_name)\n",
    "        with open(file_path, 'r', encoding='utf-8') as file:\n",
    "            # get the file id\n",
    "            file_id = os.path.splitext(file_name)[0]\n",
    "            \n",
    "            # read the file content\n",
    "            content = file.read()\n",
    "            soup = BeautifulSoup(content, 'html.parser')\n",
    "            \n",
    "            # get abstract\n",
    "            abstract_tag = soup.find('section', itemprop='abstract')\n",
    "            abstract = abstract_tag.find('div', itemprop='content').text.strip()\n",
    "            \n",
    "            # get classification\n",
    "            classification_tags = soup.find_all('span', itemprop='Code')\n",
    "            classifications = [tag.text.strip() for tag in classification_tags if len(tag.text.strip()) > 4]\n",
    "            classifications = list(set(classifications))  # delete duplicates\n",
    "            if len(classifications) == 0:\n",
    "                continue\n",
    "\n",
    "            # get timeline\n",
    "            timeline_tags = soup.find_all('dd', itemprop='events')\n",
    "            timeline_dict = {}\n",
    "            for timeline_tag in timeline_tags:\n",
    "                date = timeline_tag.find('time', itemprop='date').text.strip()\n",
    "                event = timeline_tag.find('span', itemprop='title').text.strip()\n",
    "                timeline_dict[date] = event\n",
    "            \n",
    "            # get cited by table\n",
    "            cited_by_tags = soup.find_all('tr', itemprop='forwardReferencesOrig')\n",
    "            cited_by_dict = {}\n",
    "            for cited_by_tag in cited_by_tags:\n",
    "                publication_number = cited_by_tag.find('span', itemprop='publicationNumber').text.strip()\n",
    "                if not publication_number.startswith('US'):\n",
    "                    continue  # if not a US patent, skip\n",
    "                assignee = cited_by_tag.find('span', itemprop='assigneeOriginal').text.strip()\n",
    "                cited_by_dict[publication_number] = assignee\n",
    "\n",
    "            # get legal events table\n",
    "            legal_events_tags = soup.find_all('tr', itemprop='legalEvents')\n",
    "            legal_events_dict = {}\n",
    "            for legal_event_tag in legal_events_tags:\n",
    "                code = legal_event_tag.find('td', itemprop='code').text.strip()\n",
    "                if (code.startswith('AS') and len(code) == 4) or code.startswith('PS'):\n",
    "                    date = legal_event_tag.find('time', itemprop='date').text.strip()\n",
    "                    title = legal_event_tag.find('td', itemprop='title').text.strip()\n",
    "                    legal_events_dict[date] = title\n",
    "\n",
    "            # insert the data into the database\n",
    "            cursor.execute(\"INSERT INTO patent_data (id, abstract, classification, timeline, citedby, legal) VALUES (?, ?, ?, ?, ?, ?)\",\n",
    "                           (file_id, abstract, ', '.join(classifications), str(timeline_dict), str(cited_by_dict), str(legal_events_dict)))\n",
    "\n",
    "# close the connection\n",
    "conn.commit()\n",
    "conn.close()\n",
    "\n",
    "print(\"data inserted successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# connect to the database\n",
    "conn = sqlite3.connect('patent.db')\n",
    "cursor = conn.cursor()\n",
    "\n",
    "# execute a query\n",
    "cursor.execute(\"SELECT * FROM patent_data\")\n",
    "\n",
    "# consult the results\n",
    "rows = cursor.fetchall()\n",
    "\n",
    "# print the results\n",
    "for row in rows:\n",
    "    print(row)\n",
    "\n",
    "# close the connection\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
